{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {}
   },
   "source": [
    "# Character Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you will learn about information entropy and how to compute it for discrete probability distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {}
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "nbgrader": {}
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "nbgrader": {}
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {}
   },
   "source": [
    "## Character counting and entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {}
   },
   "source": [
    "Write a function, `char_probs`, that takes a string and computes the probabilities of each character in the string:\n",
    "\n",
    "* First do a character count and store the result in a dictionary.\n",
    "* Then divide each character count by the total number of character to compute the normalized probabilties.\n",
    "* Return the dictionary of characters (keys) and probabilities (values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "d2eb2952e226cd826dc2497c2e467a4e",
     "grade": false,
     "grade_id": "charentropy-a",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def char_probs(s):\n",
    "    \"\"\"Find the probabilities of the unique characters in the string s.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    s : str\n",
    "        A string of characters.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    probs : dict\n",
    "        A dictionary whose keys are the unique characters in s and whose values\n",
    "        are the probabilities of those characters.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    probs = {}\n",
    "    count = {}\n",
    "    \n",
    "    for c in s:\n",
    "        if c not in count:\n",
    "            count[c] = 1\n",
    "        else:\n",
    "            count[c] += 1\n",
    "    \n",
    "    for i in count:\n",
    "        probs[i] = count[i]/len(s)  \n",
    "        \n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "2f65c597aa0e6b26448d1705844b995f",
     "grade": true,
     "grade_id": "charentropy-b",
     "points": 2
    }
   },
   "outputs": [],
   "source": [
    "test1 = char_probs('aaaa')\n",
    "assert np.allclose(test1['a'], 1.0)\n",
    "test2 = char_probs('aabb')\n",
    "assert np.allclose(test2['a'], 0.5)\n",
    "assert np.allclose(test2['b'], 0.5)\n",
    "test3 = char_probs('abcd')\n",
    "assert np.allclose(test3['a'], 0.25)\n",
    "assert np.allclose(test3['b'], 0.25)\n",
    "assert np.allclose(test3['c'], 0.25)\n",
    "assert np.allclose(test3['d'], 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {}
   },
   "source": [
    "The [entropy](http://en.wikipedia.org/wiki/Entropy_%28information_theory%29) is a quantiative measure of the disorder of a probability distribution. It is used extensively in Physics, Statistics, Machine Learning, Computer Science and Information Science. Given a set of probabilities $P_{i}$, the entropy is defined as:\n",
    "\n",
    "$$H = - \\Sigma_i P_i \\log_2(P_i)$$ \n",
    "\n",
    "In this expression $\\log_2$ is the base 2 log (`np.log2`), which is commonly used in information science. In Physics the natural log is often used in the definition of entropy.\n",
    "\n",
    "Write a funtion, `entropy`, that computes the entropy of a probability distribution. The probability distribution will be passed as a Python `dict`. The keys in the `dict` will be the unique values in the distribution and the values in the `dict` will be the probabilities for those keys.\n",
    "\n",
    "To compute the entropy, you should:\n",
    "\n",
    "* First convert the values (probabilities) of the `dict` to a Numpy array of probabilities.\n",
    "* Then use other Numpy functions (`np.log2`, etc.) to compute the entropy.\n",
    "* Don't use any `for` or `while` loops in your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "12226fbc7137f6c663871522f1ed33b1",
     "grade": false,
     "grade_id": "charentropy-c",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def entropy(d):\n",
    "    \"\"\"Compute the entropy of a dict d whose values are probabilities.\"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    if type(d) == str:\n",
    "        d = char_probs(d)\n",
    "    \n",
    "    probs = np.array(list(d.values()))\n",
    "    logs = np.log2(probs)\n",
    "    tot = sum([logs[i] * probs[i] for i in range(len(probs))])\n",
    "    return - tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "079879ec2852c80bd220881786861825",
     "grade": true,
     "grade_id": "charentropy-d",
     "points": 2
    }
   },
   "outputs": [],
   "source": [
    "assert np.allclose(entropy({'a': 0.5, 'b': 0.5}), 1.0)\n",
    "assert np.allclose(entropy({'a': 1.0}), 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {}
   },
   "source": [
    "Use IPython's `interact` function to create a user interface that allows you to type a string into a text box and see the entropy of the character probabilities of the string. Try to get an intuitive sense of the entropy by typing in different strings. What strings make the entropy large? Small?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "66300b08f85c10f03912bf45a06a9150",
     "grade": true,
     "grade_id": "charentropy-e",
     "locked": false,
     "points": 2,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "c = interact(entropy, d=\"string\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the entropies of the all of the books in the directory `/data/gutenberg`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/gutenberg/11.txt    /data/gutenberg/17208.txt  /data/gutenberg/33511.txt\r\n",
      "/data/gutenberg/1342.txt  /data/gutenberg/23.txt     /data/gutenberg/84.txt\r\n",
      "/data/gutenberg/1400.txt  /data/gutenberg/2701.txt\r\n",
      "/data/gutenberg/1661.txt  /data/gutenberg/29021.txt\r\n"
     ]
    }
   ],
   "source": [
    "ls /data/gutenberg/*.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Store your entropies in a list\n",
    "* Create an appropriately labeled visualization of the distribution of entropies\n",
    "* Print the mean entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "a7000be8395b1ff8ff5b67ff00df4f1d",
     "grade": true,
     "grade_id": "charentropy-f",
     "locked": false,
     "points": 4,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.49528586213\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "books = list(['/data/gutenberg/11.txt', '/data/gutenberg/17208.txt', '/data/gutenberg/33511.txt',\n",
    "'/data/gutenberg/1342.txt', '/data/gutenberg/23.txt', '/data/gutenberg/84.txt',\n",
    "'/data/gutenberg/1400.txt', '/data/gutenberg/2701.txt',\n",
    "'/data/gutenberg/1661.txt', '/data/gutenberg/29021.txt'])\n",
    "\n",
    "entropies = []\n",
    "for b in books:\n",
    "    f = open(b, 'r')\n",
    "    guten = [line for line in f]\n",
    "    guten = \"\".join(guten)\n",
    "    entropies.append(entropy(char_probs(guten)))\n",
    "\n",
    "print(np.mean(entropies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
